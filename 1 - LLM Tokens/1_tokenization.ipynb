{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "jZn7jci2U_7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, Lets open the sample txt file and check if it works by running 100 words"
      ],
      "metadata": {
        "id": "oLPeM4CHUSIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLCDB4BtTWZu",
        "outputId": "77534dea-49fa-4d03-e7c8-bb54933d259e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(\"Total number of characters: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to tokenize this 20,479-character short story into individual words and special\n",
        "characters that we can then turn into embeddings for LLM training."
      ],
      "metadata": {
        "id": "7CnXKExZVjla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the re.split command with the following syntax to split a text on whitespace characters:"
      ],
      "metadata": {
        "id": "Iief3XxkV5ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clhC8E5MWxvP",
        "outputId": "5d7180a0-9df1-4777-8459-8d4579265219"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This splits the whole sample into individual words and whitspaces(\\s)."
      ],
      "metadata": {
        "id": "ee3ap14-W0qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets modify it further to split upon other punctuation."
      ],
      "metadata": {
        "id": "lUYxcvkCXI_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1wfe_DkXIVJ",
        "outputId": "a4443041-7b2f-4873-a26d-486a960eedb9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice an issue that: even whitespaces are being included. Lets remove them."
      ],
      "metadata": {
        "id": "pZHozu0bXZHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKBCzKOYXf_-",
        "outputId": "5fc9c52e-c8c9-4cac-bab7-8779eae04311"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVE WHITESPACES OR NOT?\n",
        "\n",
        "Depends on our application and its\n",
        "requirements. Removing whitespaces reduces the memory and computing\n",
        "requirements. However, keeping whitespaces can be useful if we train models that\n",
        "are sensitive to the exact structure of the text. for example, Python code's indentation.\n"
      ],
      "metadata": {
        "id": "H0JKtZEJXwUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify tokenizer scheme bit more to include other punctuations as well."
      ],
      "metadata": {
        "id": "RMtZo3I6YHjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hSVzMUoYR9-",
        "outputId": "4dd8de50-785a-4c14-f0c9-997ef619d0f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we built a basic tokenizer working, let's apply it to Edith Wharton's entire short story:"
      ],
      "metadata": {
        "id": "k77eTq3zYlrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfI6mPIAYqPv",
        "outputId": "16ec60b4-c57d-4e08-d8df-ba87b254fcc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B74azaEBZOC1",
        "outputId": "20563a5f-dac9-4dcb-e959-caea94c64071"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Creating token IDs"
      ],
      "metadata": {
        "id": "6UxUCDeqZTwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenized the short story. Now lets convert it to unqiue tokens and sort them alphabetically."
      ],
      "metadata": {
        "id": "TXU53GhpZbi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(f\"Total number of unique words: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMjbZBGoZmb5",
        "outputId": "3089bc3c-8c0a-42f5-a54a-0d811b1fe47c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words: 1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the unique words/tokens in the story. Now lets print the first 30 ordered vocabulary"
      ],
      "metadata": {
        "id": "Nn1xx9i_Z2vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:number for number,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "RSEDEGXPaEe_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 30:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HF9IvLXeaoU0",
        "outputId": "5d9e67b3-4e1a-428d-8152-4d08c802e74a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets implement Tokenizer python class"
      ],
      "metadata": {
        "id": "RnidJb44A8M8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "    \n",
        "Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "Step 3: Process input text into token IDs\n",
        "\n",
        "Step 4: Convert token IDs back into text\n",
        "\n",
        "Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "x_Yw6RrfDfZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [ item.strip() for item in preprocessed if item.strip() ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "AgofAMocDmDJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a\n",
        "passage from Edith Wharton's short story to try it out in practice:"
      ],
      "metadata": {
        "id": "5KVm_qEwEe5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n__DuZJyEhZP",
        "outputId": "9c82d904-2a4b-4bb0-c089-fa433042596a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try decoding it"
      ],
      "metadata": {
        "id": "RXVUTIS_E2vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArrUpuXcE5Jm",
        "outputId": "b91b9901-a751-4b04-998f-7643e36af5f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, Everything works fine but what if we try encoding a word or sentence thats not present in the vocab?"
      ],
      "metadata": {
        "id": "j2rnrTw0FAl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "iIetwYPHFLix",
        "outputId": "f16797a3-e25c-4f61-8c25-122b9a3be877"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1763555282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3817026366.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word 'Hello' is not in the vocab. This highlights the need to consider large and diverse\n",
        "training sets to extend the vocabulary when working on LLMs."
      ],
      "metadata": {
        "id": "oiNRqmfRFOvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADDING SPECIAL CONTEXT TOKENS\n",
        "\n",
        "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
        "from the training set.\n",
        "\n",
        "In this section, we will modify this tokenizer to handle unknown\n",
        "words.\n",
        "\n",
        "\n",
        "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
        "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and\n",
        "<|endoftext|>"
      ],
      "metadata": {
        "id": "2FEjYUG6FfMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First lets modify the vocab to include these two new tokens"
      ],
      "metadata": {
        "id": "7HE_mRY0FlIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "ocvK3hthFqxY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmnoJ2InFt2J",
        "outputId": "01c12c68-e0b7-4e6b-99e0-b265625cfbb0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the output, the vocab size has gone up by 2. Lets verify it by printing the last 5 tokens."
      ],
      "metadata": {
        "id": "hbE7iDrQFvqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMukoo0aGXy1",
        "outputId": "14a1dccf-e6dd-4095-c820-a56707134227"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets implement another tokenizer class which includes and uses the two tokens."
      ],
      "metadata": {
        "id": "TdcjGE7LGivH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "jP4wut7UGq1G"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now comes '<|endoftext|>' token"
      ],
      "metadata": {
        "id": "Nzu9nf-7GwFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8I-mmheG64G",
        "outputId": "607c1576-af4e-4a58-c362-2631f2c7019c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try encoding it?"
      ],
      "metadata": {
        "id": "qivxzieWG-kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3RdzGqVHAiz",
        "outputId": "20a9dae4-b385-4ab1-8886-5dda7b7f9333"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try decoding it now to see"
      ],
      "metadata": {
        "id": "FQr17fnwHGF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Wrc_ykHFHIax",
        "outputId": "ed12b4aa-4469-4521-e669-c62c1856aacc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the LLM, some researchers also consider additional special tokens such\n",
        "as the following:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
        "signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
        "and is especially useful when concatenating multiple unrelated texts,\n",
        "similar to <|endoftext|>. For instance, when combining two different\n",
        "Wikipedia articles or books, the [EOS] token indicates where one article\n",
        "ends and the next one begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
        "the batch might contain texts of varying lengths. To ensure all texts have\n",
        "the same length, the shorter texts are extended or \"padded\" using the\n",
        "[PAD] token, up to the length of the longest text in the batch."
      ],
      "metadata": {
        "id": "ETs_FqqJHOma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
        "above but only uses an <|endoftext|> token for simplicity.\n",
        "The tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
        "down words into subword units"
      ],
      "metadata": {
        "id": "n4EpviSPHXDK"
      }
    }
  ]
}