{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING TOKEN EMBEDDINGS"
      ],
      "metadata": {
        "id": "X8c6lssh9VGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets install everything first\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "! pip3 install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK8Cpk6P-Q4Q",
        "outputId": "aae52ed2-acfe-4f10-ecc6-18f5dff03620"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's illustrate how the token ID to embedding vector conversion works. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:"
      ],
      "metadata": {
        "id": "tekLospI9ZQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uzUJBFnd8Fpn"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of simplicity, suppose we have a small vocabulary of\n",
        "only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want\n",
        "to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):"
      ],
      "metadata": {
        "id": "8xXA1xfn9eqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch,\n",
        "setting the random seed to 123 for reproducibility purposes:\n",
        "\n",
        "    \n",
        "</div>"
      ],
      "metadata": {
        "id": "W3TpI7L29xMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "ottrbCQ7-HUB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print statement in the code prints the embedding layer's underlying\n",
        "weight matrix:"
      ],
      "metadata": {
        "id": "uRPCV8Bc-tAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR0sKwRc-vRn",
        "outputId": "ccf96066-09c5-406d-bf8a-6168b2348100"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the weight matrix of the embedding layer contains small, random values.\n",
        "These values are optimized during LLM training as part of the LLM optimization itself, as we\n",
        "will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows\n",
        "and three columns. There is one row for each of the six possible tokens in the vocabulary.\n",
        "And there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "JsMpCT7d-zt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the\n",
        "embedding vector:"
      ],
      "metadata": {
        "id": "RIHPyOlC--Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1BX0qy3_Ajn",
        "outputId": "558d6765-f0d9-4b7f-bffc-7dcfa23edbdb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero index, so it's the row corresponding to index 3). In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n"
      ],
      "metadata": {
        "id": "Xe0U1jwx_H8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we have seen how to convert a single token ID into a three-dimensional\n",
        "embedding vector. Let's now apply that to all four input IDs we defined earlier\n",
        "(torch.tensor([2, 3, 5, 1])):"
      ],
      "metadata": {
        "id": "2PvirXr__Xwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "H2LU5ChqFGbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in this output matrix is obtained via a lookup operation from the embedding\n",
        "weight matrix"
      ],
      "metadata": {
        "id": "PqjGnqqNFIWK"
      }
    }
  ]
}