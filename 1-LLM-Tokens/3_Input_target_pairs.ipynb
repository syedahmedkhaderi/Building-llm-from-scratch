{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### CREATING INPUT-TARGET PAIRS"
      ],
      "metadata": {
        "id": "QFbOJL27QfKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets first import the BPE tokenizer"
      ],
      "metadata": {
        "id": "royqtG8Gapqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDxjJ-p-asqz",
        "outputId": "1e2f9190-ddc9-49cd-f49e-eea4d74cd03e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets instantiate the BPE tokenizer class"
      ],
      "metadata": {
        "id": "e1rUXCy6a6QH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "odcceHWMa91h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach."
      ],
      "metadata": {
        "id": "B05RGcg0QlYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, we will first tokenize the whole The Verdict short story we worked with\n",
        "earlier using the BPE tokenizer introduced in the previous section:"
      ],
      "metadata": {
        "id": "FReSPo73Qikx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiXme3z4Qn5F",
        "outputId": "f575d364-58d3-4288-d61b-1383fcaa0d50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
        "results in a slightly more interesting text passage in the next steps:"
      ],
      "metadata": {
        "id": "BfKr1fvkbeDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XnS0o9FLQIb-"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens\n",
        "and y contains the targets, which are the inputs shifted by 1:"
      ],
      "metadata": {
        "id": "UAXfa7Qabr7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context size determines how many tokens are included in the input"
      ],
      "metadata": {
        "id": "WD6CvN03tICs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52CNTDFPtKC5",
        "outputId": "46e9f620-902d-4750-ac0f-e95052f536cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the inputs along with the targets, which are the inputs shifted by one position,\n",
        "we can then create the next-word prediction tasks as\n",
        "follows:"
      ],
      "metadata": {
        "id": "Gvhxq38CtW3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5I20Bo7tXoA",
        "outputId": "985de5a1-6a97-49a7-d1ec-e7c31455c058"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token\n",
        "ID on the right side of the arrow represents the target token ID that the LLM is supposed to\n",
        "predict."
      ],
      "metadata": {
        "id": "QAMAnAKYtuYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For illustration purposes, let's repeat the previous code but convert the token IDs into"
      ],
      "metadata": {
        "id": "MAo189oNtW1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_1elHhuusXf",
        "outputId": "3e306aaa-7a14-4c68-80da-06373d854d38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've now created the input-target pairs that we can turn into use for the LLM training in\n",
        "upcoming chapters."
      ],
      "metadata": {
        "id": "FcuSn7zCuxus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that\n",
        "iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which\n",
        "can be thought of as multidimensional arrays."
      ],
      "metadata": {
        "id": "ZlT00vwduylG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In particular, we are interested in returning two tensors: an input tensor containing the\n",
        "text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
      ],
      "metadata": {
        "id": "PYR0aczou9eV"
      }
    }
  ]
}